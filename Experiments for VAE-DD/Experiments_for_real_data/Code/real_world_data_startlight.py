# -*- coding: utf-8 -*-
"""Real_world_data_startlight.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EEpkvLaqGfnDEyxMKmtzzGLxY1w0Tkaw
"""

from abc import ABC
import numpy as np
import pandas as pd
import json
import os
from sympy import binomial as comb
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras import Model, Sequential
from tensorflow.keras.layers import Dense
from collections import OrderedDict
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from itertools import combinations
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import backend as K
from timeit import default_timer as timer
from skimage.io import imread
from skimage.metrics import mean_squared_error
from random import seed, shuffle
from sklearn.ensemble import RandomForestClassifier
from scipy import stats
import matplotlib as mpl

# Define methods
class Sampling(layers.Layer):
    """Uses (z_mean, z_log_var) to sample z, the vector encoding a digit."""

    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

def Vae(data_dim,params):    
    # Build the encoder
    n_layers = len(params)-1
    encoder_inputs = keras.Input(shape=(data_dim,))
    x = encoder_inputs
    for i in range(n_layers-1):
        x = layers.Dense(params['x'+str(i+1)],activation="relu")(x)

    z_mean = layers.Dense(params['x'+str(n_layers)],name="z_mean")(x)
    z_log_var = layers.Dense(z_mean.shape[1],name="z_log_var")(x)
    z = Sampling()([z_mean, z_log_var])
    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name="encoder")
    # encoder.summary()


    # Build the decoder
    latent_inputs = keras.Input(shape=(z_mean.shape[1],))
    y = latent_inputs
    for i in range(n_layers-1):
        y = layers.Dense(params['x'+str(n_layers-i-1)],activation="relu")(y)

    decoder_outputs = layers.Dense(data_dim,activation="sigmoid")(y)
    decoder = keras.Model(latent_inputs, decoder_outputs, name="decoder")   


    vae_outputs = decoder(encoder(encoder_inputs)[2])
    vae = Model(encoder_inputs, vae_outputs, name='vae')


    reconstruction_loss = keras.losses.mean_squared_error(encoder_inputs, vae_outputs)
    reconstruction_loss *= data_dim
    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)
    kl_loss = K.sum(kl_loss, axis=-1)
    kl_loss *= -0.5
    vae_loss = K.mean(reconstruction_loss + kl_loss)
    vae.add_loss(vae_loss)
    return encoder, decoder, vae

class FDDVA(ABC):

    def __init__(self, delta, num_epochs,training_size, batch_size, win_size, data_dim):
        self.num_epochs = num_epochs
        self.training_size = training_size
        self.win_size = win_size
        self.batch_size = batch_size
        self.detection_window = []
        self.training_window = []
        self.drift_status = False
        self.delta = delta
        self.hoeffding_bound = None
        self.drift_threshold = None
        self.drifts = []
        self.counter = 0
        self.model = None
        self.scaler = None
        self.instance_counter = 0
        self.exception_threshold = None
        self.p_train = None
        self.d = data_dim
        self.anomaly_rate = 0
        self.train_preds = None
        self.detection_preds = None

    @staticmethod
    def get_predictions(model_name, x_test_scaled, threshold):
        z_mean, z_log_var, z = model_name[0].predict(x_test_scaled)

        predictions = model_name[1](z)
        # provides losses of individual instances
        errors = np.mean((x_test_scaled-predictions)**2,axis=1)
        # 0 = anomaly, 1 = normal

        if errors.size == 1:
            anomaly_mask = errors > threshold
            if anomaly_mask:
                preds = 0.0
            else:
                preds = 1.0
        else:
            anomaly_mask = pd.Series(errors) > threshold
            preds = anomaly_mask.map(lambda x: 0.0 if x else 1.0)
        return preds, errors

    @staticmethod
    def find_threshold(model_name, x_train_scaled):
       
        z_mean, _, z = model_name[0].predict(x_train_scaled)
        reconstructions = model_name[1](z)
        reconstruction_error = np.mean((x_train_scaled-reconstructions)**2,axis=1)
        threshold = np.quantile(reconstruction_error, 0.9)
        return threshold

    # Esitimate anomaly rate.
    @staticmethod
    def mad(points, thresh=3.5):
        """
        Returns a boolean array with True if points are outliers and False 
        otherwise.

        Parameters:
        -----------
            points : An numobservations by numdimensions array of observations
            thresh : The modified z-score to use as a threshold. Observations with
                a modified z-score (based on the median absolute deviation) greater
                than this value will be classified as exceptions.

        Returns:
        --------
            mask : A numobservations-length boolean array.

        References:
        ----------
            Boris Iglewicz and David Hoaglin (1993), "Volume 16: How to Detect and
            Handle Outliers", The ASQC Basic References in Quality Control:
            Statistical Techniques, Edward F. Mykytka, Ph.D., Editor. 
        """
        if len(points.shape) == 1:
            points = points[:,None]
        median = np.median(points, axis=0)
        diff = np.sum((points - median)**2, axis=-1)
        diff = np.sqrt(diff)
        med_abs_deviation = np.median(diff)

        modified_z_score = 0.6745 * diff / med_abs_deviation

        return modified_z_score > thresh

    def training_autoencoder(self, train_data, d, params):
        train_data = pd.DataFrame(train_data).reset_index(drop=True)
        scaler_1 = MinMaxScaler(feature_range=(0, 1))
        x_train_scaled_1 = scaler_1.fit_transform(train_data)
        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)
        model_1 = Vae(d,params)
        model_1[2].compile(
            optimizer=keras.optimizers.Adam(learning_rate = params['lr']),
            loss='mean_squared_error',
            metrics=['mean_squared_error'])
        model_1[2].fit(x_train_scaled_1,x_train_scaled_1,epochs=self.num_epochs, batch_size=self.batch_size, verbose=False,callbacks=[callback], validation_split=0.2)
        threshold_1 = self.find_threshold(model_1, x_train_scaled_1)
        preds_1,reconstruction_error_1 = self.get_predictions(model_1, x_train_scaled_1, threshold_1) 
        anomaly_index = np.where(self.mad(reconstruction_error_1))[0]
        anomaly_rate = anomaly_index.size/self.training_size
        clean_data = train_data.drop(anomaly_index)
        scaler = MinMaxScaler(feature_range=(0, 1))
        x_train_scaled = scaler.fit_transform(clean_data)
        model = Vae(d,params)
        model[2].compile(
            optimizer=keras.optimizers.Adam(learning_rate = params['lr']),
            loss='mean_squared_error',
            metrics=['mean_squared_error'])
        model[2].fit(x_train_scaled,x_train_scaled, epochs=self.num_epochs, batch_size=batch_size, verbose=False,callbacks=[callback],validation_split=0.2)
        self.counter += 1
        threshold = self.find_threshold(model, x_train_scaled)
        preds,reconstruction_error_clean = self.get_predictions(model, x_train_scaled, threshold)      
        self.model = model
        self.scaler = scaler
        self.exception_threshold = threshold
        self.train_preds = preds
        p_train = 1 - preds.sum() / clean_data.shape[0]
        self.anomaly_rate = max(anomaly_rate, self.anomaly_rate)
        self.p_train = p_train
        return (anomaly_index, reconstruction_error_1, threshold_1, x_train_scaled,threshold,reconstruction_error_clean)
    
    def detect(self, instance):
        self.instance_counter += 1
        data_point = self.scaler.transform(np.array(instance).reshape(1, self.d))
        pred, error = self.get_predictions(self.model, data_point, self.exception_threshold)
        if self.drift_status:
            if len(self.training_window) < self.training_size:
                self.training_window.append(instance)
            else:
                self.training_autoencoder(self.training_window, self.d, params)
                self.drift_status = False
                self.training_window = []
        elif len(self.detection_window) < self.win_size:
            self.detection_window.append(pred)
        else:
            p_now = 1 - np.sum(self.detection_window) / self.win_size
            self.detection_preds = self.detection_window
            m = 1/(1/len(self.train_preds)+1/len(self.detection_preds))
            var = np.std(list(self.train_preds)+list(self.detection_preds))**2
            self.hoeffding_bound = np.sqrt(
            2/m *var* np.log(2 / self.delta))+2/(3*m)*np.log(2 / self.delta)
            self.drift_threshold = self.hoeffding_bound+self.anomaly_rate
#             print(f'p_now:{p_now},p_train:{self.p_train}, anomaly_rate:{self.anomaly_rate},threshold:{self.drift_threshold}')
#             print(abs(p_now - self.p_train))
            if abs(p_now - self.p_train) > self.drift_threshold:
                self.drifts.append(self.instance_counter)
                print(f'Drift was detected at index:{self.instance_counter}')
                self.drift_status = True
                self.detection_window = []
            else:
                self.detection_window.append(pred)
                self.detection_window.pop(0)


font = {'family' : 'normal',
        'weight' : 'normal',
        'size'   : 13}

mpl.rc('font', **font)

mpl.rcParams['figure.figsize']=(6,4)    #(6.0,4.0)
mpl.rcParams['font.size']=12                #10 
mpl.rcParams['savefig.dpi']=100             #72 
mpl.rcParams['figure.subplot.bottom']=.11    #.125


def IBDD(train_X, test_X, window_length, consecutive_values):

    files2del = ['w1.jpeg', 'w2.jpeg', 'w1_cv.jpeg', 'w2_cv.jpeg']
    print(test_X.shape)
    n_runs = 20
    superior_threshold, inferior_threshold, nrmse = find_initial_threshold(train_X, window_length, n_runs)
    threshold_diffs = [superior_threshold - inferior_threshold]

    recent_data_X = train_X.iloc[-window_length:].copy()

    drift_points = []
    w1 = get_imgdistribution("w1.jpeg", recent_data_X)
    lastupdate = 0
    start = timer()
    print('IBDD Running...')
    for i in range(test_X.shape[0]): 
        recent_data_X.drop(recent_data_X.index[0], inplace=True, axis=0)
        recent_data_X = recent_data_X.append(test_X.iloc[[i]], ignore_index=True)

        w2 = get_imgdistribution("w2.jpeg", recent_data_X)

        nrmse.append(mean_squared_error(w1,w2))

        if (i-lastupdate > 60):
            superior_threshold = np.mean(nrmse[-50:])+2*np.std(nrmse[-50:])
            inferior_threshold = np.mean(nrmse[-50:])-2*np.std(nrmse[-50:])
            threshold_diffs.append(superior_threshold-inferior_threshold)
            lastupdate = i

        if (all(i >= superior_threshold for i in nrmse[-consecutive_values:])):
            superior_threshold = nrmse[-1] + np.std(nrmse[-50:-1])
            inferior_threshold = nrmse[-1] - np.mean(threshold_diffs)
            threshold_diffs.append(superior_threshold-inferior_threshold)
            drift_points.append(i)
            print(f'drift was detected at {i}')
            lastupdate = i

        elif (all(i <= inferior_threshold for i in nrmse[-consecutive_values:])):
            inferior_threshold = nrmse[-1] - np.std(nrmse[-50:-1])
            superior_threshold = nrmse[-1] + np.mean(threshold_diffs) 
            threshold_diffs.append(superior_threshold-inferior_threshold) 
            drift_points.append(i)
            print(f'drift was detected at {i}')
            lastupdate = i
    return drift_points


def find_initial_threshold(X_train, window_length, n_runs):
    if window_length > len(X_train):
        window_length = len(X_train)

    w1 = X_train.iloc[-window_length:].copy()
    w1_cv = get_imgdistribution("w1_cv.jpeg", w1)

    max_index = X_train.shape[0]
    sequence = [i for i in range(max_index)]
    nrmse_cv = []
    for i in range(0,n_runs):
        # seed random number generator
        seed(i)
        # randomly shuffle the sequence
        shuffle(sequence)
        w2 = X_train.iloc[sequence[:window_length]].copy()
        w2.reset_index(drop=True, inplace=True)
        w2_cv = get_imgdistribution("w2_cv.jpeg", w2)
        

        nrmse_cv.append(mean_squared_error(w1_cv,w2_cv))
        threshold1 = np.mean(nrmse_cv)+2*np.std(nrmse_cv)
        threshold2 = np.mean(nrmse_cv)-2*np.std(nrmse_cv)
    if threshold2 < 0:
        threshold2 = 0
    return (threshold1, threshold2, nrmse_cv)


def get_imgdistribution(name_file, data):
    plt.imsave(name_file, data.transpose(), cmap = 'Greys', dpi=100)
    w = imread(name_file)
    return w

def AutoEncoder(data_dim,params):
    """
  Parameters
  ----------
  output_units: int
    Number of output units

  code_size: int
    Number of units in bottle neck
  """
    n_layers = len(params)-1
    encoder_inputs = tf.keras.Input(shape=(data_dim,))
    x = encoder_inputs
    for i in range(n_layers-1):
        x = Dense(params['x'+str(i+1)],activation="relu")(x)
    latent_layer = Dense(params['x'+str(n_layers)],activation="relu")(x)
    # encoder.summary()

    # Build the decoder
    y = latent_layer
    for i in range(n_layers-1):
        y = Dense(params['x'+str(n_layers-i-1)],activation="relu")(y)

    decoder_outputs = Dense(data_dim,activation="sigmoid")(y)
    
    model = tf.keras.Model(inputs=encoder_inputs, outputs=decoder_outputs, name="autoencoder")
    return model


class ADD_ABRUPT(ABC):

    def __init__(self, num_epochs, batch_size, win_size,data_dim):
        self.num_epochs = num_epochs
        self.batch_size = batch_size
        self.win_size = win_size
        self.training_size = 300
        self.detection_window = []
        self.training_window = []
        self.train_error = None
        self.current_error = None
        self.previous_error = None
        self.instance_counter = 0
        self.abrupt_drift_status = False
        self.model = None
        self.scaler = None
        self.threshold = None
        self.abrupt_threshold = 0.005
        self.d = data_dim
        self.drifts = []
        
    @staticmethod
    def get_predictions(model_name, x_test_scaled, threshold):
        predictions = model_name.predict(x_test_scaled)
        # provides losses of individual instances
        errors = tf.keras.losses.mse(predictions, x_test_scaled)
        # 0 = anomaly, 1 = normal
        errors = errors.numpy()

        if errors.size == 1:
            anomaly_mask = errors > threshold
            if anomaly_mask:
                preds = 0.0
            else:
                preds = 1.0
        else:
            anomaly_mask = pd.Series(errors) > threshold
            preds = anomaly_mask.map(lambda x: 0.0 if x else 1.0)
        return preds, errors

    @staticmethod
    def find_threshold(model_name, x_train_scaled):
        reconstructions = model_name.predict(x_train_scaled)
        # provides losses of individual instances
        reconstruction_errors = tf.keras.losses.mse(reconstructions, x_train_scaled)

        # threshold for anomaly scores
        threshold_param = np.mean(reconstruction_errors.numpy()) + 3 * np.std(reconstruction_errors.numpy())
        return threshold_param
    
    def training_autoencoder(self, training_data, data_dim,params):
        scaler = MinMaxScaler(feature_range=(0, 1))
        train_size = training_data.shape[0]
        x_train_scaled = scaler.fit_transform(training_data)
        optimizer = tf.keras.optimizers.Adam(learning_rate=params['lr'])
        model = AutoEncoder(data_dim,params)
        model.compile(loss='mse', metrics=['mse'], optimizer=optimizer)
        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)
        model.fit(
            x_train_scaled,
            x_train_scaled,
            epochs=self.num_epochs,
            batch_size=self.batch_size,
            callbacks=[callback],
            verbose=False
        )
        threshold = self.find_threshold(model, x_train_scaled)
        _, errors = self.get_predictions(model, x_train_scaled, threshold)
        self.train_error = np.mean(errors)
        self.model = model
        self.scaler = scaler
        self.threshold = threshold

    def detect(self, instance):
        self.instance_counter += 1
        if self.abrupt_drift_status:
            if len(self.training_window) < self.training_size:
                self.training_window.append(instance)
            else:
                self.training_autoencoder(pd.DataFrame(self.training_window), self.d, params)
                self.detection_window = self.training_window
                # print(f'Retrain autoencoder and its new train_error is {self.train_error}')
                self.abrupt_drift_status = False
                self.training_window = []
        elif len(self.detection_window) < self.win_size:
            self.detection_window.append(instance)
        else:
            scaled_window = self.scaler.transform(self.detection_window)
            _, errors = self.get_predictions(self.model, scaled_window, self.threshold)
            pre_errors = np.array(errors[:int(win_size/2)])
            cur_errors = np.array(errors[int(win_size/2):])          
            self.previous_error = np.mean(pre_errors)
            self.current_error = np.mean(cur_errors)
            self.abrupt_drift_status = (self.current_error - self.previous_error) > self.abrupt_threshold
            print(f'current_error {self.current_error}')
            print(f'previous_error {self.previous_error}')
            print(f'train_error:{self.train_error}')
            if self.abrupt_drift_status:
                print(f'Abrupt drift was detected at {self.instance_counter}')
                self.drifts.append(self.instance_counter)
            self.detection_window.pop(0)
            self.detection_window.append(instance)

def AutoEncoder(data_dim,params):
    """
  Parameters
  ----------
  output_units: int
    Number of output units

  code_size: int
    Number of units in bottle neck
  """
    n_layers = len(params)-1
    encoder_inputs = tf.keras.Input(shape=(data_dim,))
    x = encoder_inputs
    for i in range(n_layers-1):
        x = Dense(params['x'+str(i+1)],activation="relu")(x)
    latent_layer = Dense(params['x'+str(n_layers)],activation="relu")(x)
    # encoder.summary()

    # Build the decoder
    y = latent_layer
    for i in range(n_layers-1):
        y = Dense(params['x'+str(n_layers-i-1)],activation="relu")(y)

    decoder_outputs = Dense(data_dim,activation="sigmoid")(y)
    
    model = tf.keras.Model(inputs=encoder_inputs, outputs=decoder_outputs, name="autoencoder")
    return model


class ADD_TREND(ABC):

    def __init__(self, num_epochs, batch_size, win_size,data_dim):
        self.num_epochs = num_epochs
        self.batch_size = batch_size
        self.win_size = win_size
        self.training_size = 200
        self.detection_window = []
        self.training_window = []
        self.d = data_dim
        self.instance_counter = 0
        self.model = None
        self.scaler = None
        self.threshold = None
        self.drift_status = False
        self.d = data_dim
        self.drifts = []
        self.C = 0
        self.TC = 0
        self.T = 0
        self.T_2 = 0
        self.n = 0
        self.t = 0
        self.forgetting_factor = 0.998
        self.drift_threshold = 0.00002
    def reset(self):
        self.C = 0
        self.TC = 0
        self.T = 0
        self.T_2 = 0
        self.n = 0
        self.t = 0
        self.detection_window = []   
        
    @staticmethod
    def get_predictions(model_name, x_test_scaled, threshold):
        predictions = model_name.predict(x_test_scaled)
        # provides losses of individual instances
        errors = tf.keras.losses.mse(predictions, x_test_scaled)
        # 0 = anomaly, 1 = normal
        errors = errors.numpy()

        if errors.size == 1:
            anomaly_mask = errors > threshold
            if anomaly_mask:
                preds = 0.0
            else:
                preds = 1.0
        else:
            anomaly_mask = pd.Series(errors) > threshold
            preds = anomaly_mask.map(lambda x: 0.0 if x else 1.0)
        return preds, errors

    @staticmethod
    def find_threshold(model_name, x_train_scaled):
        reconstructions = model_name.predict(x_train_scaled)
        # provides losses of individual instances
        reconstruction_errors = tf.keras.losses.mse(reconstructions, x_train_scaled)

        # threshold for anomaly scores
        threshold_param = np.mean(reconstruction_errors.numpy()) + 3 * np.std(reconstruction_errors.numpy())
        return threshold_param
    
    def training_autoencoder(self, training_data, data_dim,params):
        scaler = MinMaxScaler(feature_range=(0, 1))
        train_size = training_data.shape[0]
        x_train_scaled = scaler.fit_transform(training_data)
        optimizer = tf.keras.optimizers.Adam(learning_rate=params['lr'])
        model = AutoEncoder(data_dim,params)
        model.compile(loss='mse', metrics=['mse'], optimizer=optimizer)
        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)
        model.fit(
            x_train_scaled,
            x_train_scaled,
            epochs=self.num_epochs,
            batch_size=self.batch_size,
            callbacks=[callback],
            verbose=False
        )
        threshold = self.find_threshold(model, x_train_scaled)
        _, errors = self.get_predictions(model, x_train_scaled, threshold)
        self.model = model
        self.scaler = scaler
        self.threshold = threshold

    def detect(self, instance):
        print(f'Index:{self.instance_counter}')
        self.instance_counter += 1
        data_point = self.scaler.transform(np.array(instance).reshape(1, self.d))
        pred, error = self.get_predictions(self.model, data_point, self.threshold)
        if self.drift_status:
            if len(self.training_window) < self.training_size:
                self.training_window.append(instance)
            else:
                self.training_autoencoder(pd.DataFrame(self.training_window), self.d, params)
                self.drift_status = False
                self.training_window = []
        elif len(self.detection_window)< self.win_size:
            self.detection_window.append(error)
        else:
            self.t = self.t+1
            self.detection_window.pop(0)
            self.detection_window.append(error)
            mean_error = np.mean(self.detection_window)
            C_m = mean_error
            self.C = self.forgetting_factor *self.C+C_m
            self.TC = self.forgetting_factor *self.TC+self.t*C_m
            self.T = self.forgetting_factor *self.T+self.t
            self.T_2 = self.forgetting_factor *self.T_2+self.t**2
            self.n = self.forgetting_factor *self.n+1
            if self.t>=2:
                Q_c = (self.n*self.TC-self.T*self.C)/(self.n*self.T_2 - self.T**2)
                print(Q_c)
                if Q_c > self.drift_threshold:
                    self.drift_status = True
                    print(f'Drift was detected at index:{self.instance_counter}')
                    self.drifts.append(self.instance_counter)
                    self.reset()
                    self.detection_window.append(error)

# Drift detection
stream_train = pd.read_csv('StarLightCurves_TRAIN_with_anomaly.csv',header=None)
stream = pd.read_csv('StarLightCurves_TEST_with_anomaly.csv',header=None)
stream_train = stream_train.iloc[:,:-1]
stream = stream.iloc[:,:-1]
np.random.seed(0)
tf.random.set_seed(0)
learning_rate = 0.001
num_epochs = 500
training_size = 300
batch_size = 64
win_size = 100
delta = 0.05
dim = stream.shape[1]
params = {'lr': learning_rate, 'x1': int(np.ceil(dim*0.8)), 'x2': int(np.ceil(dim*0.4)), 'x3': int(np.ceil(dim*0.2))}

add_trend = ADD_TREND(num_epochs, batch_size, win_size, dim)
add_trend.training_autoencoder(stream_train.iloc[:300,],dim, params)
for i in range(stream.shape[0]):
    add_trend.detect(stream.iloc[i,])

fddva = FDDVA(delta,num_epochs,training_size, batch_size, win_size, dim)
fddva.training_autoencoder(stream_train.iloc[:300,:],dim,params)
for i in range(stream.shape[0]):
    fddva.detect(stream.iloc[i,])

add_abrupt = ADD_ABRUPT(num_epochs, batch_size, win_size, dim)
add_abrupt.training_autoencoder(stream_train.iloc[:300,:],dim, params)
for i in range(stream.shape[0]):
    add_abrupt.detect(stream.iloc[i,])

add_star = add_abrupt.drifts
add_trend_star = add_trend.drifts
fddva_star = fddva.drifts

training_size = 300
window_size = 300 #window parameter to build the images for comparison
epsilon = 3 #number of MSD values above/below threshold
drifts = IBDD(stream_train,stream,window_size, epsilon)
ibdd_start = drifts